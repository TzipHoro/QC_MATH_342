---
title: "Lab 3"
author: "Tziporah Horowitz"
output: pdf_document
date: "11:59PM Saturday, February 22, 2020"
---

## Review from Lab 2

You have set of names divided by gender (M / F) and generation (Boomer / GenX / Millenial):

* M / Boomer      "Theodore, Bernard, Gene, Herbert, Ray, Tom, Lee, Alfred, Leroy, Eddie"
* M / GenX        "Marc, Jamie, Greg, Darryl, Tim, Dean, Jon, Chris, Troy, Jeff"
* M / Millennial  "Zachary, Dylan, Christian, Wesley, Seth, Austin, Gabriel, Evan, Casey, Luis"
* F / Boomer      "Gloria, Joan, Dorothy, Shirley, Betty, Dianne, Kay, Marjorie, Lorraine, Mildred"
* F / GenX        "Tracy, Dawn, Tina, Tammy, Melinda, Tamara, Tracey, Colleen, Sherri, Heidi"
* F / Millennial  "Samantha, Alexis, Brittany, Lauren, Taylor, Bethany, Latoya, Candice, Brittney, Cheyenne"

Create a list-within-a-list that will intelligently store this data.

```{r}
set <- list("M" = list(), "F" = list())
set$M$Boomer = c("Theodore", "Bernard", "Gene", "Herbert", "Ray", "Tom",
                 "Lee", "Alfred", "Leroy", "Eddie")
set$M$GenX = c("Marc", "Jamie", "Greg", "Darryl", "Tim", "Dean", "Jon", 
               "Chris", "Troy", "Jeff")
set$M$Millennial = strsplit("Zachary, Dylan, Christian, Wesley, Seth, Austin, Gabriel, Evan, Casey, Luis", split = ", ")[[1]]
set$F$Boomer = strsplit("Gloria, Joan, Dorothy, Shirley, Betty, Dianne, Kay, Marjorie, Lorraine, Mildred", split = ", ")[[1]]
set$F$GenX = strsplit("Tracy, Dawn, Tina, Tammy, Melinda, Tamara, Tracey, Colleen, Sherri, Heidi", split = ", ")[[1]]
set$F$Millennial = strsplit("Samantha, Alexis, Brittany, Lauren, Taylor, Bethany, Latoya, Candice, Brittney, Cheyenne", split = ", ")[[1]]

set
```


Imagine you are running an experiment with many manipulations. You have 14 levels in the variable "treatment" with levels a, b, c, etc. For each of those manipulations you have 3 submanipulations in a variable named "variation" with levels A, B, C. Then you have "gender" with levels M / F. Then you have "generation" with levels Boomer, GenX, Millenial. Then you will have 6 runs per each of these groups. In each set of 6 you will need to select a name without duplication from the appropriate set of names (from the last question). Create a data frame with columns treatment, variation, gender, generation, name and y that will store all the unique unit information in this experiment. Leave y empty because it will be measured as the experiment is executed.

```{r}
n <- 14*3*2*3*10
X <- data.frame(treatment = rep(NA, n),
                variation = rep(NA, n),
                gender = rep(NA, n),
                generation = rep(NA, n),
                name = rep(NA, n),
                y = rep(NA, n))
X$treatment <- rep(letters[1:14], each = n/14)
X$variation <- rep(rep(LETTERS[1:3], each = n/(14*3)), 14)
X$gender <- rep(rep(c("M", "F"), each = n/(14*3*2)), 14*3)
X$generation <- rep(rep(c("Boomer", "GenX", "Millennial"), each = n/(14*3*2*3)), 14*3*2)
X$name <- rep(unlist(set), 14*3)
tail(X, 50)
```


## Packages

Install the package `pacman` using regular base R.

```{r}
#install.packages("pacman")
```


First, install the package `testthat` (a widely accepted testing suite for R) from https://github.com/r-lib/testthat using `pacman`. If you are using Windows, this will be a long install, but you have to go through it for some of the stuff we are doing in class. LINUX (or MAC) is preferred for coding. If you can't get it to work, install this package from CRAN (still using `pacman`), but this is not recommended long term.

```{r}
pacman::p_load(testthat)
```


* Create vector `v` consisting of all numbers from -100 to 100 and test using the second line of code:

```{r}
v <- -100:100
expect_equal(v, -100 : 100)
```


If there are any errors, the `expect_equal` function will tell you about them. If there are no errors, then it will be silent.

Test the `my_reverse` function from lab2 using the following code:

```{r}
my_reverse <- function(v){
  v[length(v):1]
}
expect_equal(my_reverse(v), rev(v))
expect_equal(my_reverse(c("A", "B", "C")), rev(LETTERS[1:3]))
```


## Basic Binary Classification Modeling

* Load the famous `iris` data frame into the namespace. Provide a summary of the columns and write a few descriptive sentences about the distributions using the code below and in English.

```{r}
data(iris)
names(iris)
unique(iris$Species)
summary(subset(iris, Species == "setosa"))
summary(subset(iris, Species == "versicolor"))
summary(subset(iris, Species == "virginica"))
```


The `iris` dataset is used to desribe and identify three species of flowers: setosa, versicolor, and virginica. To do so, it uses measurements of sepal length, sepal width, petal length, and petal width for 50 of each species. The data suggests that out of the three species, virginicas tend to be largest in sepal width, petal length, and petal width, while setosas tend to be smallest in the three categories. However, sepal width does not seem to increase with the other measurements.

The outcome metric is `Species`. This is what we will be trying to predict. However, we only care about binary classification between "setosa" and "versicolor" for the purposes of this exercise. Thus the first order of business is to drop one class. Let's drop the data for the level "virginica" from the data frame.

```{r}
iris <- iris[iris$Species != "virginica", ]
table(iris$Species)
```


Now create a vector `y` that is length the number of remaining rows in the data frame whose entries are 0 if "setosa" and 1 if "versicolor".

```{r}
y <- ifelse(iris$Species == "setosa", 0, 1)
y
```


* Write a `Mode` function 

```{r}
Mode <- function(x){
  u <- unique(x)
  vec <- rep(0, length(u))
  for (i in 1:length(u)){
    for (j in 1:length(x)){
      if (x[j] == u[i]){
        vec[i] <- vec[i] + 1
      }
    }
  }
  m <- max(vec)
  tvec <- c()
  for (i in 1:length(vec)){
    if (vec[i] == m){
      tvec <- c(tvec, u[i])
    }
  }
  if (length(tvec) > 1) warning("warning: x is multimodal \n \n")
  tvec
}

k = c(1, 2, 2, 3, 4, 1, 1, 4, 4, 2)
Mode(k)
```


* Fit a threshold model to `y` using the feature `Sepal.Length`. Write your own code to do this. What is the estimated value of the threshold parameter? What is the total number of errors this model makes?

```{r}
n <- nrow(iris)
numErrors <- array(NA, n)
iris$Sepal.Length
for (i in 1:n){
  y_hat <- as.numeric(iris$Sepal.Length > iris$Sepal.Length[i])
  numErrors[i] <- sum(y_hat != y)
}
numErrors
threshold <- iris$Sepal.Length[which.min(numErrors)]
g <- function(x){
  as.numeric(x > threshold)
}

sum(g(iris$Sepal.Length) != y)
```


Does this make sense given the following summaries:

```{r}
summary(iris[iris$Species == "setosa", "Sepal.Length"])
summary(iris[iris$Species == "versicolor", "Sepal.Length"])
```

Write your answer here in English.

75% of versicolors have Sepal.Length greater than 5.4 while 75% of setosas have Sepal.Length less than 5.4.


Create the function `g` explicitly that can predict `y` from `x` being a new `Sepal.Length`.

```{r}
g <- function(x){
  ifelse(x > threshold, 1, 0)
}
g(iris$Sepal.Length)
```


* What is the total number of errors this model makes in the dataset $\mathbb{D}$?

```{r}
sum(g(iris$Sepal.Length) != y)
```


## Perceptron

You will code the "perceptron learning algorithm". Take a look at the comments above the function. This is standard "Roxygen" format for documentation. Hopefully, we will get to packages at some point and we will go over this again. It is your job also to fill in this documentation.

```{r}
#' Perceptron Learning Algorithm
#'
#' Assuming the data is linearly seperable, the perceptron fits the best line for the data.
#'
#' @param Xinput      The training data features as a n x p matrix
#' @param y_binary    The training data responses as a n x 1 vector of 1's and 0's
#' @param MAX_ITER    The maximum number of iterations the algorithm performs, default = 1000
#' @param w           (p + 1) x 1 vector of weights, initialized as NULL
#'
#' @return            The computed final parameter (weight) as a vector of length p + 1
#' @export            [In a package, this documentation parameter signifies this function becomes a public method. Leave this blank.]
#'
#' @author            [Tziporah Horowitz]
perceptron_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 1000, w = NULL){
  
  n <- nrow(Xinput)
  Xinput <- cbind(rep(1, n), Xinput)
  p <- ncol(Xinput)
  
  if (is.null(w)) {
    w <- rep(0, p)
  }
  
  for (i in 1:MAX_ITER) {
    for (j in 1:n) {
      X <- Xinput[j, ]
      e <- y_binary[j] - ifelse(sum(w %*% X) > 0, 1, 0)
      for (k in 1:p) {
        w[k] <- w[k] + e * X[k]
      }
    }
  }
  return(w)
}
```


To understand what the algorithm is doing - linear "discrimination" between two response categories, we can draw a picture. First let's make up some very simple training data $\mathbb{D}$.

```{r}
Xy_simple = data.frame(
 response = factor(c(0, 0, 0, 1, 1, 1)), #nominal
 first_feature = c(1, 1, 2, 3, 3, 4),    #continuous
 second_feature = c(1, 2, 1, 3, 4, 3)    #continuous
)
```


We haven't spoken about visualization yet, but it is important we do some of it now. Thus, I will write this code for you and you will just run it. First we load the visualization library we're going to use:

```{r}
pacman::p_load(ggplot2)
```


We are going to just get some plots and not talk about the code to generate them as we will have a whole unit on visualization using `ggplot2` in the future.

Let's first plot $y$ by the two features so the coordinate plane will be the two features and we use different colors to represent the third dimension, $y$.

```{r}
simple_viz_obj = ggplot(Xy_simple, aes(x = first_feature, y = second_feature, color = response)) + 
  geom_point(size = 5)
simple_viz_obj
```

The graph shows binary responses based on the first feature and the second feature.


Now, let us run the algorithm and see what happens:

```{r}
w_vec_simple_per = perceptron_learning_algorithm(
  cbind(Xy_simple$first_feature, Xy_simple$second_feature),
  as.numeric(Xy_simple$response == 1))
w_vec_simple_per
```


Explain this output. What do the numbers mean? What is the intercept of this line and the slope? You will have to do some algebra.

The intercept is -7 and the slope is 4. Responses above the line x2 = 4x1 + 7 will be 0 and responses below the line will be 1.


```{r}
simple_perceptron_line = geom_abline(
    intercept = -w_vec_simple_per[1] / w_vec_simple_per[3], 
    slope = -w_vec_simple_per[2] / w_vec_simple_per[3], 
    color = "orange", na.rm = TRUE)
simple_viz_obj + simple_perceptron_line
```

Explain this picture. Why is this line of separation not "satisfying" to you?

The perceptron line is not the "best" way to separate the data. 


For extra credit, program the maximum-margin hyperplane perceptron that provides the best linear discrimination model for linearly separable data. Make sure you provide ROxygen documentation for this function.

```{r}
#TO-DO
```


## Support Vector Machine


```{r}
X_simple_feature_matrix = as.matrix(Xy_simple[, 2 : 3])
y_binary = as.numeric(Xy_simple$response == 1)
```


Use the `e1071` package to fit an SVM model to `y_binary` using the features in `X_simple_feature_matrix`. Do not specify the $\lambda$ (i.e. do not specify the `cost` argument). Call the model object `svm_model`. Otherwise the remaining code won't work.

```{r}
svm_model = e1071::svm(X_simple_feature_matrix, Xy_simple$response, kernel = "linear", scale = FALSE)
```


and then use the following code to visualize the line in purple:

```{r}
w_vec_simple_svm = c(
  svm_model$rho, #the b term
  -t(svm_model$coefs) %*% X_simple_feature_matrix[svm_model$index, ] # the other terms
)
simple_svm_line = geom_abline(
    intercept = -w_vec_simple_svm[1] / w_vec_simple_svm[3], 
    slope = -w_vec_simple_svm[2] / w_vec_simple_svm[3], 
    color = "purple")
simple_viz_obj + simple_perceptron_line + simple_svm_line
```

Is this SVM line a better fit than the perceptron?

Yes.


3. Now write pseuocode for your own implementation of the linear support vector machine algorithm using the Vapnik objective function we discussed.

Note there are differences between this spec and the perceptron learning algorithm spec in question \#1. You should figure out a way to respect the `MAX_ITER` argument value. 


```{r}
#' Support Vector Machine 
#
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
  # she <- 0
  # for (i in 1:nrow(i)){
  #   she <- sum + max(0, 0.5 - (y_binary[i] - 0.5) * (w %*% Xinput[i, ] - b))
  # }
  # med <- min(she/n + lambda*(norm(w)^2))
  # med
} 
```


If you are enrolled in 390 the following is extra credit but if you're enrolled in 650, the following is required. Write the actual code. You may want to take a look at the `optimx` package we discussed in class. You can feel free to define another function (a "private" function) in this chunk if you wish. R has a way to create public and private functions, but I believe you need to create a package to do that (beyond the scope of this course).

```{r}
#' This function implements the hinge-loss + maximum margin linear support vector machine algorithm of Vladimir Vapnik (1963).
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param MAX_ITER    The maximum number of iterations the algorithm performs. Defaults to 5000.
#' @param lambda      A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss.
#'                    The default value is 1.
#' @return            The computed final parameter (weight) as a vector of length p + 1
linear_svm_learning_algorithm = function(Xinput, y_binary, MAX_ITER = 5000, lambda = 0.1){
  #TO-DO
}
```


If you wrote code (the extra credit), run your function using the defaults and plot it in brown vis-a-vis the previous model's line:

```{r}
# svm_model_weights = linear_svm_learning_algorithm(X_simple_feature_matrix, y_binary)
# my_svm_line = geom_abline(
#     intercept = svm_model_weights[1] / svm_model_weights[3],#NOTE: negative sign removed from intercept argument here
#     slope = -svm_model_weights[2] / svm_model_weights[3],
#     color = "brown")
# simple_viz_obj  + my_svm_line
```

Is this the same as what the `e1071` implementation returned? Why or why not?


4. Write a $k=1$ nearest neighbor algorithm using the Euclidean distance function. Respect the spec below:

```{r}
#' This function implements the nearest neighbor algorithm.
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param Xtest       The test data that the algorithm will predict on as a n* x p matrix.
#' @return            The predictions as a n* length vector.
nn_algorithm_predict = function(Xinput, y_binary, Xtest){
  n <- nrow(Xinput)
  distances <- c()
  min <- 1
  for(i in 1:n){
    distances <- c(distances, sum((Xinput[i, ] - Xtest)^2))
    if (distances[i] < distances[min]) {
      min <- i
    }
  }
  y_binary[min]
}
```


Write a few tests to ensure it actually works:

```{r}
nn_algorithm_predict(X_simple_feature_matrix, y_binary, c(1, 2))
nn_algorithm_predict(X_simple_feature_matrix, y_binary, c(1, 1))
nn_algorithm_predict(X_simple_feature_matrix, y_binary, c(4, 3))
```


We now add an argument `d` representing any legal distance function to the `nn_algorithm_predict` function. Update the implementation so it performs NN using that distance function. Set the default function to be the Euclidean distance in the original function. Also, alter the documentation in the appropriate places.

```{r}
#' This function implements the nearest neighbor algorithm.
#'
#' @param Xinput      The training data features as an n x p matrix.
#' @param y_binary    The training data responses as a vector of length n consisting of only 0's and 1's.
#' @param Xtest       The test data that the algorithm will predict on as a n* x p matrix.
#' @param d           The distance metric with parameters Xinput[i, ] and Xtest, default = Euclidean. 
#' @return            The predictions as a n* length vector.
nn_algorithm_predict_d = function(Xinput, y_binary, Xtest, 
                                  d = function(Xin, Xt){
                                    sum((Xin - Xt)^2)
                                  }){
  n <- nrow(Xinput)
  distances <- c()
  min <- 1
  for(i in 1:n){
    distances <- c(distances, d(Xinput[i, ], Xtest))
    if(distances[i] < distances[min]) {
      min <- i
    }
  }
  y_binary[min]
}
```


For extra credit (unless you're a masters student), add an argument `k` to the `nn_algorithm_predict` function and update the implementation so it performs KNN. In the case of a tie, choose $\hat{y}$ randomly. Set the default `k` to be the square root of the size of $\mathcal{D}$ which is an empirical rule-of-thumb popularized by the "Pattern Classification" book by Duda, Hart and Stork (2007). Also, alter the documentation in the appropriate places.

```{r}
#TO-DO --- extra credit for undergrads
```


