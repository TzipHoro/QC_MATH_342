---
title: "Lab 7"
author: "Tziporah Horowitz"
output: pdf_document
date: "7:09PM April 8, 2020"
---

### NOTE: I did this lab using rstudio cloud because I didn't have a computer, so a lot of chunks would not run and I wasn't able to test a lot of the code
  
Run three OLS models on the boston housing data using all available features: 
  
(1) where the response is medv, 
(2) where the response is the log base 10 of medv and
(3) where the response is the square root of medv. 

Compare the two models on oos se of the residuals. Use K = 5 to create a training-test split. Which model is better? 
  
```{r}
pacman::p_load(MASS)
boston <- Boston

n <- nrow(boston)
K <- 5

test_indices <- sample(1 : n, 1 / K * n)
train_indices <- setdiff(1 : n, test_indices)

X <- boston[, 1:13]
y <- boston$medv

X_train <- X[train_indices, ]
y_train <- y[train_indices]
X_test <- X[test_indices, ]
y_test <- y[test_indices]

mod1 <- lm(y_train ~ ., X_train)
mod2 <- lm(log10(y_train) ~ ., X_train)
mod3 <- lm(sqrt(y_train) ~ ., X_train)

yhat_oos1 <- predict(mod1, X_test)
yhat_oos2 <- predict(mod2, X_test)
yhat_oos3 <- predict(mod3, X_test)

oos_residuals1 = y_test - yhat_oos1
oos_residuals2 = y_test - yhat_oos2
oos_residuals3 = y_test - yhat_oos3

sd(oos_residuals1)
sd(oos_residuals2)
sd(oos_residuals3)

```

When evaluating the models out of sample, did you ever extrapolate? Which predictions specifically in your test set were extrapolations? How "bad" were the exrapolations? 
  
```{r}

```

Regardless of the model that came out better, lets consider the response to be raw medv i.e. without taking a transformation. Run a model that includes all squared features (except `chas` which is binary). Does this model do better than vanilla OLS from question 1?

```{r}
colnames(boston)
mod4 <- lm(medv ~ poly(crim, 2) + poly(zn, 2) + poly(indus, 2) + chas + poly(nox, 2) +
             poly(rm, 2) + poly(age, 2) + poly(dis, 2) + poly(rad, 2) + poly(tax, 2) +
             poly(ptratio, 2) + poly(black, 2) + poly(lstat, 2), boston)

summary(mod4)$sigma
summary(mod4)$r.squared
```

Run a model that includes all polynomial functions of degree 3 of all features (except `chas` which is binary). Does this model do better than the degree 2 polynomial function of the previous question?

```{r}
mod5 <- lm(medv ~ poly(crim, 3) + poly(zn, 3) + poly(indus, 3) + chas + poly(nox, 3) +
             poly(rm, 3) + poly(age, 3) + poly(dis, 3) + poly(rad, 3) + poly(tax, 3) +
             poly(ptratio, 3) + poly(black, 3) + poly(lstat, 3), boston)

summary(mod5)$sigma
summary(mod5)$r.squared
```

Use polynomial regression to perfectly fitting the following data:

```{r}
n = 10
set.seed(1984)
x = runif(n, 0, 10)
y = 5 + 2 * x + rnorm(n)

mod6 <- y ~ poly(x, 9)
summary(lm(mod6))
```

Illustrate Runge's phenomenon in this model by scatterplotting the data with $g(x)$ overlaid in green.

```{r warning=FALSE}
pacman::p_load(ggplot2)

ggplot(, aes(x = x, y = y)) + geom_point() + geom_smooth(method = lm, formula = mod6, col = "green")
```

For the rest of this assignment, I highly recommend using the [ggplot cheat sheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) as a reference resource. You will see questions that say "Create the best-looking plot". Among other things you may choose to do, remember to label the axes using real English, provide a title, subtitle. You may want to pick a theme and color scheme that you like and keep that constant throughout this lab. The default is fine if you are running short of time.

Load up the `GSSvocab` dataset in package `carData` as `X` and drop all observations with missing measurements. Briefly summarize the documentation on this dataset. What is the data type of each variable? What is the response variable?
  
```{r}
pacman::p_load(ggthemes)
pacman::p_load(carData)

X <- na.omit(GSSvocab)

str(X)
summary(X)
```

Create two different plots and identify the best-looking plot you can to examine the `age` variable. Save the best looking plot as an appropriately-named PDF.

```{r}
ggplot(X) + geom_point(aes(x = age, y = educ, col = vocab, shape = gender)) +
  ylab("Education (Years)") +
  scale_color_gradient2_tableau() +
  theme_minimal()

ggplot(X, aes(x = age)) + 
  geom_histogram(aes(y=..density..), alpha = .6, binwidth = 3, na.rm = TRUE) +
  geom_density(alpha = .3, fill = "red") +
  xlim(0, 100) +
  xlab("Age") +
  ggtitle("Age Distribution") +
  geom_vline(xintercept = mean(X$age), col = "pink", linetype = "dashed") +
  theme_minimal()
ggsave("age.pdf")
```

Create two different plots and identify the best looking plot you can to examine the `vocab` variable. Save the best looking plot as an appropriately-named PDF.

```{r}
ggplot(X) + geom_col(aes(x = vocab, y = educ, fill = educGroup)) +
  xlab("Vocabulary") +
  ylab("") +
  ggtitle("Vocabulary by Education") +
  labs(fill = "Level of Education") +
  scale_fill_brewer(palette = "PuRd") +
  theme_minimal()

ggplot(X) + 
  geom_jitter(aes(x = vocab, y = age, col = educGroup)) +
  xlab("Vocabulary") +
  ylab("Age") +
  ggtitle("Vocabulary by Age and Education") +
  scale_color_brewer(palette = "BrBG") +
  theme_minimal()
ggsave("vocab.pdf")
```

Create the best-looking plot you can to examine the `ageGroup` variable by `gender`. Does there appear to be an association? There are many ways to do this.

```{r}
ggplot(X) + geom_bar(aes(x = ageGroup,  fill = gender)) +
  xlab("Education (Years)") +
  ylab("") +
  ggtitle("Distribution of Gender in Age Groups")
  theme_minimal()
```

Create the best-looking plot you can to examine the `vocab` variable by `age`. Does there appear to be an association?
  
```{r}
ggplot(X) + geom_col(aes(x = vocab, y = age, fill = ageGroup)) +
  xlab("Vocabulary") +
  ylab("") +
  ggtitle("Vocabulary by Age") +
  labs(fill = "Age Group") +
  scale_fill_brewer(palette = "GnBu") +
  theme_minimal()
  
ageplot <- ggplot(X, aes(x = vocab, y = age)) + 
  geom_point() +
  xlab("Vocabulary") +
  ylab("Age") +
  ggtitle("Vocabulary by Age")
ageplot
```

Add an estimate of $f(x)$ using the smoothing geometry to the previous plot. Does there appear to be an association now?
  
```{r}
ageplot + geom_smooth(method = "lm")
```

Using the plot from the previous question, create the best looking overloading with variable `gender`. Does there appear to be an interaction of `gender` and `age`?
  
```{r}
ageplot + aes(col = gender) + ggtitle("Vocabulary by Age and Gender")
```


Using the plot from the previous question, create the best looking overloading with variable `nativeBorn`. Does there appear to be an interaction of `nativeBorn` and `age`?
  
```{r}
ageplot + aes(col = nativeBorn) + ggtitle("Vocabulary by Age and Navtive Born")
```

Create two different plots and identify the best-looking plot you can to examine the `vocab` variable by `educGroup`. Does there appear to be an association?
  
```{r}
ggplot(X) + 
  geom_jitter(aes(x = vocab, y = educGroup, col = nativeBorn)) +
  xlab("Vocabulary") +
  ylab("Education Level") +
  ggtitle("Vocabulary by Education Level") +
  theme_minimal()

ggplot(X) + geom_bar(aes(x = vocab, fill = educGroup)) +
  xlab("Vocabulary") +
  ylab("") +
  ggtitle("Vocabulary by Education") +
  labs(fill = "Level of Education") +
  scale_fill_brewer(palette = "RdBu") +
  theme_minimal()
```

Using the best-looking plot from the previous question, create the best looking overloading with variable `gender`. Does there appear to be an interaction of `gender` and `educGroup`?
  
```{r}
ggplot(X) + 
  geom_jitter(aes(x = vocab, y = educGroup, col = gender)) +
  xlab("Vocabulary") +
  ylab("Education Level") +
  ggtitle("Vocabulary by Education Level") +
  theme_minimal()

```

Using facets, examine the relationship between `vocab` and `ageGroup`. You can drop year level `(Other)`. Are we getting dumber?
  
```{r}
ggplot(X) + 
  geom_bar(aes(x = vocab, fill = ageGroup)) +
  facet_wrap(~ year) +
  scale_fill_brewer(palette = "PuBuGn")
  
```

We will now be getting some experience with speeding up R code using C++ via the `Rcpp` package.

First, clear the workspace and load the `Rcpp` package.

```{r}
rm(list = ls())
pacman::p_load(Rcpp)
```

Create a variable `n` to be 10 and a variable `Nvec` to be 100 initially. Create a random vector via `rnorm` `Nvec` times and load it into a `Nvec` x `n` dimensional matrix.

```{r}
n <- 10
Nvec <- 100
X <- c()
for (i in 1:n){
  x <- rnorm(Nvec)
  X <- cbind(X, x)
}

dim(X)
```

Write a function `all_angles` that measures the angle between each of the pairs of vectors. You should measure the vector on a scale of 0 to 180 degrees with negative angles coerced to be positive.

```{r}
all_angles <- function(X, n){
angle <- c()

for (i in 1:(n-1)){
  theta_in_rad <- acos((X[ , i] %*% X[ , i+1]) / (sqrt(sum(X[ , i]^2)) * sqrt(sum(X[ , i+1]^2))))
  angle <- c(angle, theta_in_rad * 180 / pi)
}
angle
}
```

Plot the density of these angles.

```{r}
pacman::p_load(ggplot2)

ggplot() + geom_density(aes(x = angle))
```

Write an Rcpp function `all_angles_cpp` that does the same thing. Use an IDE if you want, but write it below in-line.

```{r}
# I think these function need pointers to work but they don't teach that in CS 111
cppFunction('void mmult(double matrix1[1][100], double matrix2[100][1]){
                // initialize entries
                int r1 = 1, r2 = 100, c1 = 1, c2 = 100;
                double mult[r1][c2] = {0};
                // matrix multiplication
                for(int i = 0; i < r1; ++i){
                  for(int j = 0; j < c2; ++j){
                    for(int k = 0; k < c1; ++k){
                      mult[i][j] += matrix1[i][k] * matrix2[k][j];
                    }
                  }
                }
                return mult;
              }')
cppFunction('double angleCPP(double arr[100][10]){
              int dim1 = 100, dim2 = 10;
              double temp1[dim1][1], temp2[dim1][1], angle[dim2-1];
              double theta_in_rad;
              double sumsq1 = 0, sumsq2 = 0;
              double angle[dim2 - 1] = {0};
              for (int j = 0; j < (n-1); j++){
                for (int i = 0; i < dim1; i++){
                  temp1 = arr[i][j];
                  temp2 = arr[i][j+1];
                  sumsq1 += temp1[i][j]^2; 
                  sumsq2 += temp1[i][j+1]^2; 
                }
                theta_in_rad = acos(mmult(temp1, temp2) / (sumsq1 * sumsq2));
                angle[j] = theta_in_rad * (180 / 3.1415926535897)
              }
              return angle;
            }')
```

Test the time difference between these functions for `n = 1000` and `Nvec = 100, 500, 1000, 5000`.  Store the results in a matrix.

```{r}
n <- 1000
Nvec <- c(100, 500, 1000, 5000)
for (i in 1:length(Nvec)){
  X <- c()
  for (j in 1:n){
    x <- rnorm(Nvec[i])
    X <- cbind(X, x)
  }
  system.time({
    anglesr = all_angles(X, n)
  })
  system.time({
    anglescpp = angleCPP(X)
  })
}
```

Plot the divergence of performance (in log seconds) over n using a line geometry. Use two different colors for the R and CPP functions. Make sure there's a color legend on your plot.

```{r}
pacman::p_load(ggplot2)
ggplot() +
  geom_abline(log(anglesr), col = "red") + geom_abline(log(anglescpp), col = green)
```

Let `Nvec = 10000` and vary `n` to be 10, 100, 1000. Plot the density of angles for all three values of `n` on one plot using color to signify `n`. Make sure you have a color legend. This is not easy.

```{r}
Nvec <= 10000
X <- c()
for (i in 1:10){
  x <- rnorm(Nvec)
  X <- cbind(X, x)
}
ang1 <- all_angles(X, 10)
X <- c()
for (i in 1:100){
  x <- rnorm(Nvec)
  X <- cbind(X, x)
}
ang2 <- all_angles(X, 100)
X <- c()
for (i in 1:1000){
  x <- rnorm(Nvec)
  X <- cbind(X, x)
}
ang3 <- all_angles(X, 1000)

ggplot() + 
  geom_density(aes(x = ang1, fill = "red"), alpha = .4) +
  geom_density(aes(x = ang2, fill = "blue"), alpha = .4) +
  geom_density(aes(x = ang3, fill = "green"), alpha = .4) +
  scale_fill_discrete(labels = c("n=10", "n=100", "n=1000"))
```

Write an R function `nth_fibonnaci` that finds the nth Fibonnaci number via recursion but allows you to specify the starting number. For instance, if the sequence started at 1, you get the familiar 1, 1, 2, 3, 5, etc. But if it started at 0.01, you would get 0.01, 0.01, 0.02, 0.03, 0.05, etc.

```{r}
nth_fibonacci <- function(n, start){
  if (n == 0 | n == 1) return(start)
  else return(nth_fibonacci(n, start-1) + nth_fibonacci(n, start-2))
}
nth_fibonacci(3, 0.01)
```

Write an Rcpp function `nth_fibonnaci_cpp` that does the same thing. Use an IDE if ou want, but write it below in-line.

```{r}
cppFunction(
  'double nth_fibonacci_cpp(int n, double start){
    if (n == 0 || n == 1) return start;
    else return (nth_fibonacci_cpp(n, start-1) + nth_fibonacci_cpp(n, start-2));
  }'
)
```

Time the difference in these functions for n = 100, 200, ...., 1500 while starting the sequence at the smallest possible floating point value in R. Store the results in a matrix.

```{r}
system.time(
  {fibr = nth_fibonacci(4, 1)}
)
system.time(
  {fibcpp = nth_fibonacci_cpp(4, 1)}
)

```

Plot the divergence of performance (in log seconds) over n using a line geometry. Use two different colors for the R and CPP functions. Make sure there's a color legend on your plot.

```{r}
ggplot() +
  geom_abline(log(fibr), col = "red") + geom_abline(log(fibcpp), col = green)
```
